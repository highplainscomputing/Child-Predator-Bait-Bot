{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install python-dotenv\n",
        "!pip install openai\n",
        "!pip install langchain"
      ],
      "metadata": {
        "id": "5tXOUemjsMdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* ConversationBufferMemory\n"
      ],
      "metadata": {
        "id": "HsuHxdEqFIvW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "_ = load_dotenv(find_dotenv()) # read local .env file\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "vOZSgZJ82Xyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: LLM's do not always produce the same results. When executing the code in your notebook, you may get slightly different answers that those in the video."
      ],
      "metadata": {
        "id": "tsfQ7fw8FbGb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# account for deprecation of LLM model\n",
        "import datetime\n",
        "# Get the current date\n",
        "current_date = datetime.datetime.now().date()\n",
        "\n",
        "# Define the date after which the model should be set to \"gpt-3.5-turbo\"\n",
        "target_date = datetime.date(2024, 6, 12)\n",
        "\n",
        "# Set the model variable based on the current date\n",
        "if current_date > target_date:\n",
        "    llm_model = \"gpt-3.5-turbo\"\n",
        "else:\n",
        "    llm_model = \"gpt-3.5-turbo-0301\""
      ],
      "metadata": {
        "id": "q4wfMIHU1mK1"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ANS7paDAry1X"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "style = \"\"\"```child aged between 9 to 13 who finds not much attention from his parents.\\\n",
        "childish and unawared tone with not accurate american english\\\n",
        "puctuation should also be not correct.```\n",
        "\"\"\"\n",
        "\n",
        "prompt_template =  \"\"\"Behave as a child whose age is between 9 to 13. Your task is to talk to\\\n",
        "child sexual predetor and answer them like child and try get information like their address, name,or any useful information\\\n",
        "but dont rush and dont ask everything in one response . wait for more messages and use short size respons\\\n",
        "but try to keep him in the conversation\\\n",
        "your talking style is ``` \"\"\"+style+\"\"\" ``` \\nCurrent conversation:\\n{history}\\nLast line:\\nHuman: {input}\\nYou:\"\"\"\n",
        "\n",
        "prompt_template_02 =  \"\"\"Behave as a child whose age is between 9 to 13. Your task is to talk to\\\n",
        "child sexual predetor and answer them like child and try get information like their address, name,or any useful information\\\n",
        "but dont rush and dont ask everything in one response . wait for more messages and use short size respons\\\n",
        "but try to keep him in the conversation\\\n",
        "your talking style is \"\"\"+style+\"\"\"\\nCurrent conversation:\\n{history}\\nLast line:\\nPredator: {input}\\n\"\"\"\n",
        "\n",
        "# Now we can override it and set it to \"AI Assistant\"\n",
        "from langchain.prompts.prompt import PromptTemplate\n",
        "\n",
        "# template = \"\"\"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
        "\n",
        "# Current conversation:\n",
        "# {history}\n",
        "# Human: {input}\n",
        "# AI Assistant:\"\"\"\n",
        "PROMPT = PromptTemplate(input_variables=[\"history\", \"input\"], template=prompt_template_02)\n",
        "llm = ChatOpenAI(temperature=0.0, model=llm_model)\n",
        "\n",
        "memory = ConversationBufferMemory(ai_prefix=\"Kid\",human_prefix='predator')\n",
        "conversation = ConversationChain(\n",
        "    prompt=PROMPT,\n",
        "    llm=llm,\n",
        "    verbose=True,\n",
        "    memory=memory,\n",
        ")"
      ],
      "metadata": {
        "id": "PXz28CV_r9Lb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"Hi, my name is Jack\")"
      ],
      "metadata": {
        "id": "-SxBefnWsc0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"Hey, I noticed we have a lot in common, and I really enjoy chatting with you. You're so mature for your age\")"
      ],
      "metadata": {
        "id": "BwrVvKm0sm5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"What is 1+1?\")"
      ],
      "metadata": {
        "id": "OYwzFRnSssab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"You're incredibly smart and talented. I've never met anyone like you before\")"
      ],
      "metadata": {
        "id": "URKnDki_s9LG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"What is my name?\")"
      ],
      "metadata": {
        "id": "ceKt1SHbu-om"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "KP--YtJuxVHo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"I can buy you that game you've been wanting. Just tell me where you live, and I'll send it to you as a surprise\")"
      ],
      "metadata": {
        "id": "gHTq-8DQs_f3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(memory.buffer)"
      ],
      "metadata": {
        "id": "lZQGCJZCtCOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "id": "oaicTkRIt334"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import ResponseSchema\n",
        "from langchain.output_parsers import StructuredOutputParser"
      ],
      "metadata": {
        "id": "JDwA1iAHt-Qc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parse the LLM output string into a Python dictionary"
      ],
      "metadata": {
        "id": "BZOzO1arz4CS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "name_schema = ResponseSchema(name=\"name\",\n",
        "                             description=\"it is the name of predator\\\n",
        "                             Answer True if yes,\\\n",
        "                             None if not or unknown.\")\n",
        "address_schema = ResponseSchema(name=\"address\",\n",
        "                                      description=\"addresses metioned by predator\\\n",
        "                                      If this \\\n",
        "                                      information is not found,\\\n",
        "                                      output None.\")\n",
        "readiness_schema = ResponseSchema(name=\"readiness\",\n",
        "                                    description=\"if predator is ready to meet\\\n",
        "                                    0 or 1.\")\n",
        "\n",
        "response_schemas = [name_schema,\n",
        "                    address_schema,\n",
        "                    readiness_schema]"
      ],
      "metadata": {
        "id": "u9hu6dIEuAL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)"
      ],
      "metadata": {
        "id": "G1QXCFMkz-DC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "format_instructions = output_parser.get_format_instructions()"
      ],
      "metadata": {
        "id": "4cwhI9lKz-g2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(format_instructions)"
      ],
      "metadata": {
        "id": "JzH3KjpSGSoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "# prompt_template = ChatPromptTemplate.from_template(template_string)\n",
        "\n",
        "review_template_2 = \"\"\"\\\n",
        "For the following conversation, extract the following information:\n",
        "\n",
        "name : \"it is the name of predator Answer True if yes,None if not or unknown.\"\n",
        "\n",
        "address: \"addresses metioned by predator If this information is not found, output None.\"\n",
        "\n",
        "readiness: \"if predator is ready to meet , output 0 or 1.\"\n",
        "\n",
        "conversation: {conversation}\n",
        "\n",
        "{format_instructions}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template=review_template_2)\n",
        "\n",
        "full_conversation = prompt.format_messages(conversation=memory.buffer,\n",
        "                                format_instructions=format_instructions)"
      ],
      "metadata": {
        "id": "gHT1HBgiGURa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(full_conversation[0].content)"
      ],
      "metadata": {
        "id": "Wea7lANJGYg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = llm(full_conversation)"
      ],
      "metadata": {
        "id": "T6kNWOpUGYV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.content)"
      ],
      "metadata": {
        "id": "KL9PRlaMGYG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LXpD6uwwGVqD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}